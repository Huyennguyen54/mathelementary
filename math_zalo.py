# -*- coding: utf-8 -*-
"""Math_Zalo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rQbBdZaz3lqVIyVyajsbz3KoqlCLBlqk
"""

!gdown --id 1y53EsKCX2B4XNFV74sbyibUKgfAm34Td

!pip install -q datasets
!pip install -q -U accelerate
!pip install -q -U transformers
!pip install -q sentencepiece
!pip install peft

import pandas as pd
import os
import json
import torch
from typing import Optional, Union
import numpy as np
from datasets import Dataset
from dataclasses import dataclass
from transformers import AutoTokenizer
from transformers import EarlyStoppingCallback
from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy
from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer

with open("math_train.json") as json_file:
    json_data = json.load(json_file)
    print(json_data['data'])

dict_one_hot_answer = {
    0: "A",
    1: "B",
    2: "C",
    3: "D"
}

list_question = []
list_answer = []
list_A = []
list_B = []
list_C = []
list_D = []
list_explanation = []

for record in json_data['data']:
  question = record['question']
  choices = record['choices']
  try:
    explanation = record['explanation']
  except KeyError:
    explanation = "None"
  answer = record['answer']

  list_A.append(choices[0])
  list_B.append(choices[1])
  list_C.append(choices[2])
  try:
    list_D.append(choices[3])
  except IndexError:
    list_D.append("None")
  list_question.append(question)
  one_hot_answer = choices.index(answer)
  list_answer.append(dict_one_hot_answer[one_hot_answer])
  list_explanation.append(explanation)

data_df = pd.DataFrame(list(zip(list_question, list_explanation, list_A, list_B, list_C, list_D, list_answer)),
                       columns=['question', 'explanation', 'A', 'B', 'C', 'D', 'answer'])

data_df

VER=2
NUM_TRAIN_SAMPLES = 1_024
USE_PEFT = True
FREEZE_LAYERS = 18
FREEZE_EMBEDDINGS = True
MAX_INPUT = 256
MODEL = 'microsoft/deberta-v3-large'

option_to_index = {option: idx for idx, option in enumerate('ABCD')}
index_to_option = {v: k for k,v in option_to_index.items()}

def preprocess(example):
    first_sentence = [ "[CLS] " + example['explanation'] ] * 4
    second_sentences = [" #### " + example['question'] + " [SEP] " + example[option] + " [SEP]" for option in 'ABCD']
    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=True,
                                  max_length=MAX_INPUT, add_special_tokens=False)
    tokenized_example['label'] = option_to_index[example['answer']]

    return tokenized_example

@dataclass
class DataCollatorForMultipleChoice:
    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None

    def __call__(self, features):
        label_name = 'label' if 'label' in features[0].keys() else 'labels'
        labels = [feature.pop(label_name) for feature in features]
        batch_size = len(features)
        num_choices = len(features[0]['input_ids'])
        flattened_features = [
            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features
        ]
        flattened_features = sum(flattened_features, [])

        batch = self.tokenizer.pad(
            flattened_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors='pt',
        )
        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}
        batch['labels'] = torch.tensor(labels, dtype=torch.int64)
        return batch

from sklearn.model_selection import train_test_split

df_train, df_valid = train_test_split(data_df, test_size=0.1, random_state=42)

print(df_train.shape, df_valid.shape)

tokenizer = AutoTokenizer.from_pretrained(MODEL)
dataset_valid = Dataset.from_pandas(df_valid)
dataset = Dataset.from_pandas(df_train)
dataset = dataset.remove_columns(["__index_level_0__"])
dataset

tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['question', 'explanation', 'A', 'B', 'C', 'D', 'answer'])
tokenized_dataset = dataset.map(preprocess, remove_columns=['question', 'explanation', 'A', 'B', 'C', 'D', 'answer'])
tokenized_dataset

model = AutoModelForMultipleChoice.from_pretrained(MODEL)

if USE_PEFT:
    print('We are using PEFT.')
    from peft import LoraConfig, get_peft_model, TaskType
    peft_config = LoraConfig(
        r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.1,
        bias="none", inference_mode=False,
        target_modules=["query_proj", "value_proj"],
        modules_to_save=['classifier','pooler'],
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

if FREEZE_EMBEDDINGS:
    print('Freezing embeddings.')
    for param in model.deberta.embeddings.parameters():
        param.requires_grad = False
if FREEZE_LAYERS>0:
    print(f'Freezing {FREEZE_LAYERS} layers.')
    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:
        for param in layer.parameters():
            param.requires_grad = False

def map_at_3(predictions, labels):
    map_sum = 0
    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]
    for x,y in zip(pred,labels):
        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]
        map_sum += np.sum(z)
    return map_sum / len(predictions)

def compute_metrics(p):
    predictions = p.predictions.tolist()
    labels = p.label_ids.tolist()
    return {"map@3": map_at_3(predictions, labels)}

training_args = TrainingArguments(
    warmup_ratio=0.1,
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=2,
    num_train_epochs=20,
    report_to='none',
    output_dir = f'./checkpoints_{VER}',
    overwrite_output_dir=True,
    fp16=True,
    gradient_accumulation_steps=8,
    logging_steps=25,
    evaluation_strategy='steps',
    eval_steps=25,
    save_strategy="steps",
    save_steps=25,
    load_best_model_at_end=False,
    metric_for_best_model='map@3',
    lr_scheduler_type='cosine',
    weight_decay=0.01,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset_valid,
    compute_metrics = compute_metrics,
    # callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],
)

trainer.train()
trainer.save_model(f'model_v{VER}')

!gdown --id 1C_OJ6F-BEEvdZI1dcLTa9r5dltly9gpc

del model, trainer
if USE_PEFT:
    model = AutoModelForMultipleChoice.from_pretrained(MODEL)
    model = get_peft_model(model, peft_config)
    checkpoint = torch.load(f'model_v{VER}/pytorch_model.bin')
    model.load_state_dict(checkpoint)
else:
    model = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')
trainer = Trainer(model=model)

df_test = df_valid.drop(columns=['answer'])

def test_preprocess(example):
    first_sentence = [ "[CLS] " + example['explanation'] ] * 4
    second_sentences = [" #### " + example['question'] + " [SEP] " + example[option] + " [SEP]" for option in 'ABCD']
    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=True,
                                  max_length=MAX_INPUT, add_special_tokens=False, padding='max_length')

    return tokenized_example

tokenized_test_dataset = Dataset.from_pandas(df_test).map(
        test_preprocess, remove_columns=['question', 'explanation', 'A', 'B', 'C', 'D'])
# tokenized_dataset_valid
test_predictions = trainer.predict(tokenized_test_dataset).predictions
predictions_as_ids = np.argsort(-test_predictions, 1)
predictions_as_answer_letters = np.array(list('ABCD'))[predictions_as_ids]
predictions_as_string = df_test['prediction'] = [
    ' '.join(row) for row in predictions_as_answer_letters[:, :3]
]

import numpy as np

def precision_at_k(r, k):
    assert k <= len(r)
    assert k != 0
    return sum(int(x) for x in r[:k]) / k

def MAP_at_3(predictions, true_items):
    U = len(predictions)
    map_at_3 = 0.0
    for u in range(U):
        user_preds = predictions[u].split()
        user_true = true_items[u]
        user_results = [1 if item == user_true else 0 for item in user_preds]
        for k in range(min(len(user_preds), 3)):
            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]

    return map_at_3 / U

m = MAP_at_3(df_test.prediction.values, df_valid.answer.values)
print( 'CV MAP@3 =',m )

